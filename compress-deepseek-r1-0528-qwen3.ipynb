{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472d580-e182-4e83-91eb-cdceb7251928",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install llmcompressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "141c1cac-d2d3-4dce-a171-72ca6499d6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compressed-tensors        0.10.2\n",
      "llmcompressor             0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d3920d-3a71-42f2-ae9d-33680adead0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "664cd658-180d-4096-bde0-196125447f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prompt', 'prompt_id', 'messages', 'text']\n",
      "['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "NUM_CALIBRATION_SAMPLES=512\n",
    "MAX_SEQUENCE_LENGTH=2048\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=f\"train_sft[:{NUM_CALIBRATION_SAMPLES}]\")\n",
    "ds = ds.shuffle(seed=42)\n",
    "\n",
    "# Preprocess the data into the format the model is trained with\n",
    "def preprocess(example):\n",
    "    return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], tokenize=False,)}\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "print(\"Columns in the dataset after pro-processing:\" + ds.column_names)\n",
    "\n",
    "top_1_row = ds.select(range(1))\n",
    "for row in top_1_row:\n",
    "    print(\"Top row data for the dataset after pro-processing:\" + row)\n",
    "\n",
    "# Tokenize the data (be careful with bos tokens - we need add_special_tokens=False since the chat_template already added it)\n",
    "def tokenize(sample):\n",
    "    return tokenizer(sample[\"text\"], padding=False, max_length=MAX_SEQUENCE_LENGTH, truncation=True, add_special_tokens=False)\n",
    "ds = ds.map(tokenize, remove_columns=ds.column_names)\n",
    "\n",
    "print(\"Columns in the dataset after tokenizing and removing default dataset columns:\"ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec9800-ee01-433a-ab07-5a54bbd46fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "\n",
    "# Configure the quantization algorithm to run\n",
    "recipe = GPTQModifier(targets=\"Linear\", scheme=\"W4A16\", ignore=[\"lm_head\"])\n",
    "\n",
    "# Apply quantization\n",
    "oneshot(\n",
    "    model=model, dataset=ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    ")\n",
    "\n",
    "# Save to disk compressed\n",
    "SAVE_DIR = MODEL_ID.rstrip(\"/\").split(\"/\")[-1] + \"-W4A16-G128\"\n",
    "model.save_pretrained(SAVE_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a40d2e-9f2a-4cab-8574-791bc0a50a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a91ac84-df86-4b66-acba-c22a3200946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"<<your-token>>\") # Replace with your actual token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595ec7bb-5db4-465f-9130-20d69623c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "repo_id = \"balakriz/DeepSeek-R1-0528-Qwen3-8B-W4A16-G128\" # Replace with your desired repo ID\n",
    "api.create_repo(repo_id, exist_ok=True) # Create the repository if it doesn't exist\n",
    "\n",
    "local_model_path = \"./DeepSeek-R1-0528-Qwen3-8B-W4A16-G128\"\n",
    "\n",
    "# Push the model and tokenizer to the repository\n",
    "api.upload_folder(\n",
    "    folder_path=local_model_path,\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
